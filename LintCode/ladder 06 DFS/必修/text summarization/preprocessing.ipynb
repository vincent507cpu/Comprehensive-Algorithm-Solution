{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import jieba\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pathlib\n",
    "\n",
    "# 预处理数据 构建数据集\n",
    "is_build_dataset = True\n",
    "\n",
    "# 获取项目根目录\n",
    "root = '/Users/wenjiazhai/Documents/学习资料/开课吧/企业课/project 1/codes'\n",
    "\n",
    "# 训练数据路径\n",
    "train_data_path = os.path.join(root, 'data', 'train.csv')\n",
    "# 测试数据路径\n",
    "test_data_path = os.path.join(root, 'data', 'test.csv')\n",
    "# 停用词路径\n",
    "# stop_word_path = os.path.join(root, 'data', 'stopwords/哈工大停用词表.txt')\n",
    "stop_word_path = os.path.join(root, 'data', 'stopwords/stopwords.txt')\n",
    "\n",
    "# 自定义切词表\n",
    "user_dict = os.path.join(root, 'data', 'user_dict.txt')\n",
    "\n",
    "# 0. 预处理\n",
    "# 预处理后的训练数据\n",
    "train_seg_path = os.path.join(root, 'data', 'train_seg_data.csv')\n",
    "# 预处理后的测试数据\n",
    "test_seg_path = os.path.join(root, 'data', 'test_seg_data.csv')\n",
    "# 合并训练集测试集数据\n",
    "merger_seg_path = os.path.join(root, 'data', 'merged_train_test_seg_data.csv')\n",
    "\n",
    "# 1. 数据标签分离\n",
    "train_x_seg_path = os.path.join(root, 'data', 'train_X_seg_data.csv')\n",
    "train_y_seg_path = os.path.join(root, 'data', 'train_Y_seg_data.csv')\n",
    "\n",
    "val_x_seg_path = os.path.join(root, 'data', 'val_X_seg_data.csv')\n",
    "val_y_seg_path = os.path.join(root, 'data', 'val_Y_seg_data.csv')\n",
    "\n",
    "test_x_seg_path = os.path.join(root, 'data', 'test_X_seg_data.csv')\n",
    "\n",
    "# 2. pad oov处理后的数据\n",
    "train_x_pad_path = os.path.join(root, 'data', 'train_X_pad_data.csv')\n",
    "train_y_pad_path = os.path.join(root, 'data', 'train_Y_pad_data.csv')\n",
    "test_x_pad_path = os.path.join(root, 'data', 'test_X_pad_data.csv')\n",
    "test_y_pad_path = os.path.join(root, 'data', 'test_Y_pad_data.csv')\n",
    "\n",
    "# 3. numpy 转换后的数据\n",
    "train_x_path = os.path.join(root, 'data', 'train_X')\n",
    "train_y_path = os.path.join(root, 'data', 'train_Y')\n",
    "test_x_path = os.path.join(root, 'data', 'test_X')\n",
    "test_y_path = os.path.join(root, 'data', 'test_Y')\n",
    "\n",
    "# 词向量路径\n",
    "save_wv_model_path = os.path.join(root, 'data', 'wv', 'word2vec.model')\n",
    "# 词向量矩阵保存路径\n",
    "embedding_matrix_path = os.path.join(root, 'data', 'wv', 'embedding_matrix')\n",
    "# 字典路径\n",
    "vocab_path = os.path.join(root, 'data', 'wv', 'vocab.txt')\n",
    "reverse_vocab_path = os.path.join(root, 'data', 'wv', 'reverstest_save_dire_vocab.txt')\n",
    "\n",
    "# 词向量训练轮数\n",
    "wv_train_epochs = 5\n",
    "\n",
    "# 模型保存文件夹\n",
    "# checkpoint_dir = os.path.join(root, 'data', 'checkpoints', 'training_checkpoints_pgn_cov_not_clean')\n",
    "\n",
    "checkpoint_dir = os.path.join(root, 'data', 'checkpoints', 'training_checkpoints_pgn_cov_backed')\n",
    "\n",
    "# checkpoint_dir = os.path.join(root, 'data', 'checkpoints', 'training_checkpoints_seq2seq')\n",
    "seq2seq_checkpoint_dir = os.path.join(root, 'data', 'checkpoints', 'training_checkpoints_seq2seq')\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "# 结果保存文件夹\n",
    "save_result_dir = os.path.join(root, 'result')\n",
    "\n",
    "# 词向量维度\n",
    "embedding_dim = 300\n",
    "\n",
    "sample_total = 82871\n",
    "\n",
    "batch_size = 32\n",
    "# batch_size = 4\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "vocab_size = 30000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "# cpu 数量\n",
    "cores = cpu_count()\n",
    "# 分块个数\n",
    "partitions = cores\n",
    "\n",
    "\n",
    "def parallelize(df, func):\n",
    "    \"\"\"多核并行处理模块\n",
    "    :param df: DataFrame数据\n",
    "    :param func: 预处理函数\n",
    "    :return: 处理后的数据\n",
    "    \"\"\"\n",
    "    # 数据切分\n",
    "    data_split = np.array_split(df, partitions)\n",
    "    # 线程池\n",
    "    pool = Pool(cores)\n",
    "    # 数据分发 合并\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    # 关闭线程池\n",
    "    pool.close()\n",
    "    # 执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def save_dict(save_path, dict_data):\n",
    "    \"\"\"保存字典\n",
    "    :param save_path: 保存路径\n",
    "    :param dict_data: 字典路径\n",
    "    \"\"\"\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        for k, v in dict_data.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(k, v))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "class Vocab:\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    UNKNOWN_TOKEN = '<UNK>'\n",
    "    START_DECODING = '<START>'\n",
    "    STOP_DECODING = '<STOP>'\n",
    "    MASKS = [PAD_TOKEN, UNKNOWN_TOKEN, START_DECODING, STOP_DECODING]\n",
    "    MASK_COUNT = len(MASKS)\n",
    "\n",
    "    PAD_TOKEN_INDEX = MASKS.index(PAD_TOKEN)\n",
    "    UNKNOWN_TOKEN_INDEX = MASKS.index(UNKNOWN_TOKEN)\n",
    "    START_DECODING_INDEX = MASKS.index(START_DECODING)\n",
    "    STOP_DECODING_INDEX = MASKS.index(STOP_DECODING)\n",
    "\n",
    "    def __init__(self, vocab_file=vocab_path, vocab_max_size=None):\n",
    "        \"\"\"Vocab 对象,vocab基本操作封装\n",
    "        :param vocab_file: Vocab 存储路径\n",
    "        :param vocab_max_size: 最大字典数量\n",
    "        \"\"\"\n",
    "        self.word2id, self.id2word = self.load_vocab(vocab_file, vocab_max_size)\n",
    "        self.count = len(self.word2id)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(file_path, vocab_max_size=None):\n",
    "        \"\"\"读取字典\n",
    "        :param file_path: 文件路径\n",
    "        :param vocab_max_size:\n",
    "        :return: 返回读取后的字典\n",
    "        \"\"\"\n",
    "        vocab = {mask: index\n",
    "                 for index, mask in enumerate(Vocab.MASKS)}\n",
    "\n",
    "        reverse_vocab = {index: mask\n",
    "                         for index, mask in enumerate(Vocab.MASKS)}\n",
    "\n",
    "        for line in open(file_path, \"r\", encoding='utf-8').readlines():\n",
    "            word, index = line.strip().split(\"\\t\")\n",
    "            index = int(index)\n",
    "            # 如果vocab 超过了指定大小\n",
    "            # 跳出循环 截断\n",
    "            if vocab_max_size and index > vocab_max_size - Vocab.MASK_COUNT:\n",
    "                print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (\n",
    "                    vocab_max_size, index))\n",
    "                break\n",
    "            vocab[word] = index + Vocab.MASK_COUNT\n",
    "            reverse_vocab[index + Vocab.MASK_COUNT] = word\n",
    "        return vocab, reverse_vocab\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            return self.word2id[self.UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def id_to_word(self, word_id):\n",
    "        if word_id not in self.id2word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self.id2word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        return self.count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
